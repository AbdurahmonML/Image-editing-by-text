{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "708tjzjvvYpH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b11f1199-8277-4e65-b2fc-1550f3be88da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: aiogram==2.14 in /usr/local/lib/python3.10/dist-packages (2.14)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from aiogram==2.14) (3.10.5)\n",
            "Requirement already satisfied: Babel>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from aiogram==2.14) (2.16.0)\n",
            "Requirement already satisfied: certifi>=2020.6.20 in /usr/local/lib/python3.10/dist-packages (from aiogram==2.14) (2024.7.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.7.2->aiogram==2.14) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.7.2->aiogram==2.14) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.7.2->aiogram==2.14) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.7.2->aiogram==2.14) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.7.2->aiogram==2.14) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.7.2->aiogram==2.14) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.7.2->aiogram==2.14) (4.0.3)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.0->aiohttp<4.0.0,>=3.7.2->aiogram==2.14) (3.8)\n"
          ]
        }
      ],
      "source": [
        "from argparse import Namespace\n",
        "import time\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "!pip install aiogram==2.14"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibh3c1rx0kze",
        "outputId": "e6099442-82d7-402d-847c-bc8479c0840a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'encoder4editing' already exists and is not an empty directory.\n",
            "--2024-09-01 14:45:49--  https://github.com/ninja-build/ninja/releases/download/v1.8.2/ninja-linux.zip\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/1335132/d2f252e2-9801-11e7-9fbf-bc7b4e4b5c83?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20240901%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240901T144511Z&X-Amz-Expires=300&X-Amz-Signature=12b93079ff29f7930e2a6e327285f2bb6c945c2d95bfea459291c31512784f4e&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=1335132&response-content-disposition=attachment%3B%20filename%3Dninja-linux.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2024-09-01 14:45:49--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/1335132/d2f252e2-9801-11e7-9fbf-bc7b4e4b5c83?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20240901%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240901T144511Z&X-Amz-Expires=300&X-Amz-Signature=12b93079ff29f7930e2a6e327285f2bb6c945c2d95bfea459291c31512784f4e&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=1335132&response-content-disposition=attachment%3B%20filename%3Dninja-linux.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 77854 (76K) [application/octet-stream]\n",
            "Saving to: ‘ninja-linux.zip.4’\n",
            "\n",
            "ninja-linux.zip.4   100%[===================>]  76.03K  --.-KB/s    in 0.009s  \n",
            "\n",
            "2024-09-01 14:45:50 (8.35 MB/s) - ‘ninja-linux.zip.4’ saved [77854/77854]\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.1.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.15.4)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.5)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.7.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1cUv_reLE6k3604or78EranS7XzuVMWeO\n",
            "From (redirected): https://drive.google.com/uc?id=1cUv_reLE6k3604or78EranS7XzuVMWeO&confirm=t&uuid=2787deae-1bc6-4507-bfe3-624262a9c1b6\n",
            "To: /content/encoder4editing/model.pth\n",
            "100%|██████████| 1.20G/1.20G [00:39<00:00, 30.1MB/s]\n",
            "<ipython-input-2-acc4739f2bc3>:42: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  ckpt = torch.load(model_path, map_location='cpu')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading e4e over the pSp framework from checkpoint: model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/encoder4editing/models/psp.py:44: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  ckpt = torch.load(self.opts.checkpoint_path, map_location='cpu')\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pSp(\n",
              "  (encoder): Encoder4Editing(\n",
              "    (input_layer): Sequential(\n",
              "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): PReLU(num_parameters=64)\n",
              "    )\n",
              "    (body): Sequential(\n",
              "      (0): bottleneck_IR_SE(\n",
              "        (shortcut_layer): MaxPool2d(kernel_size=1, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "        (res_layer): Sequential(\n",
              "          (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (2): PReLU(num_parameters=64)\n",
              "          (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (5): SEModule(\n",
              "            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc1): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (relu): ReLU(inplace=True)\n",
              "            (fc2): Conv2d(4, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (sigmoid): Sigmoid()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1): bottleneck_IR_SE(\n",
              "        (shortcut_layer): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
              "        (res_layer): Sequential(\n",
              "          (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (2): PReLU(num_parameters=64)\n",
              "          (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (5): SEModule(\n",
              "            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc1): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (relu): ReLU(inplace=True)\n",
              "            (fc2): Conv2d(4, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (sigmoid): Sigmoid()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (2): bottleneck_IR_SE(\n",
              "        (shortcut_layer): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
              "        (res_layer): Sequential(\n",
              "          (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (2): PReLU(num_parameters=64)\n",
              "          (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (5): SEModule(\n",
              "            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc1): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (relu): ReLU(inplace=True)\n",
              "            (fc2): Conv2d(4, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (sigmoid): Sigmoid()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (3): bottleneck_IR_SE(\n",
              "        (shortcut_layer): Sequential(\n",
              "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (res_layer): Sequential(\n",
              "          (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (2): PReLU(num_parameters=128)\n",
              "          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (5): SEModule(\n",
              "            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc1): Conv2d(128, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (relu): ReLU(inplace=True)\n",
              "            (fc2): Conv2d(8, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (sigmoid): Sigmoid()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (4): bottleneck_IR_SE(\n",
              "        (shortcut_layer): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
              "        (res_layer): Sequential(\n",
              "          (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (2): PReLU(num_parameters=128)\n",
              "          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (5): SEModule(\n",
              "            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc1): Conv2d(128, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (relu): ReLU(inplace=True)\n",
              "            (fc2): Conv2d(8, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (sigmoid): Sigmoid()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (5): bottleneck_IR_SE(\n",
              "        (shortcut_layer): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
              "        (res_layer): Sequential(\n",
              "          (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (2): PReLU(num_parameters=128)\n",
              "          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (5): SEModule(\n",
              "            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc1): Conv2d(128, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (relu): ReLU(inplace=True)\n",
              "            (fc2): Conv2d(8, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (sigmoid): Sigmoid()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (6): bottleneck_IR_SE(\n",
              "        (shortcut_layer): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
              "        (res_layer): Sequential(\n",
              "          (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (2): PReLU(num_parameters=128)\n",
              "          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (5): SEModule(\n",
              "            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc1): Conv2d(128, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (relu): ReLU(inplace=True)\n",
              "            (fc2): Conv2d(8, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (sigmoid): Sigmoid()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (7): bottleneck_IR_SE(\n",
              "        (shortcut_layer): Sequential(\n",
              "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (res_layer): Sequential(\n",
              "          (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (2): PReLU(num_parameters=256)\n",
              "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (5): SEModule(\n",
              "            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (relu): ReLU(inplace=True)\n",
              "            (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (sigmoid): Sigmoid()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (8): bottleneck_IR_SE(\n",
              "        (shortcut_layer): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
              "        (res_layer): Sequential(\n",
              "          (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (2): PReLU(num_parameters=256)\n",
              "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (5): SEModule(\n",
              "            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (relu): ReLU(inplace=True)\n",
              "            (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (sigmoid): Sigmoid()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (9): bottleneck_IR_SE(\n",
              "        (shortcut_layer): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
              "        (res_layer): Sequential(\n",
              "          (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (2): PReLU(num_parameters=256)\n",
              "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (5): SEModule(\n",
              "            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (relu): ReLU(inplace=True)\n",
              "            (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (sigmoid): Sigmoid()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (10): bottleneck_IR_SE(\n",
              "        (shortcut_layer): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
              "        (res_layer): Sequential(\n",
              "          (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (2): PReLU(num_parameters=256)\n",
              "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (5): SEModule(\n",
              "            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (relu): ReLU(inplace=True)\n",
              "            (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (sigmoid): Sigmoid()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (11): bottleneck_IR_SE(\n",
              "        (shortcut_layer): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
              "        (res_layer): Sequential(\n",
              "          (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (2): PReLU(num_parameters=256)\n",
              "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (5): SEModule(\n",
              "            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (relu): ReLU(inplace=True)\n",
              "            (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (sigmoid): Sigmoid()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (12): bottleneck_IR_SE(\n",
              "        (shortcut_layer): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
              "        (res_layer): Sequential(\n",
              "          (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (2): PReLU(num_parameters=256)\n",
              "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (5): SEModule(\n",
              "            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (relu): ReLU(inplace=True)\n",
              "            (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (sigmoid): Sigmoid()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (13): bottleneck_IR_SE(\n",
              "        (shortcut_layer): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
              "        (res_layer): Sequential(\n",
              "          (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (2): PReLU(num_parameters=256)\n",
              "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (5): SEModule(\n",
              "            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (relu): ReLU(inplace=True)\n",
              "            (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (sigmoid): Sigmoid()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (14): bottleneck_IR_SE(\n",
              "        (shortcut_layer): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
              "        (res_layer): Sequential(\n",
              "          (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (2): PReLU(num_parameters=256)\n",
              "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (5): SEModule(\n",
              "            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (relu): ReLU(inplace=True)\n",
              "            (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (sigmoid): Sigmoid()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (15): bottleneck_IR_SE(\n",
              "        (shortcut_layer): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
              "        (res_layer): Sequential(\n",
              "          (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (2): PReLU(num_parameters=256)\n",
              "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (5): SEModule(\n",
              "            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (relu): ReLU(inplace=True)\n",
              "            (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (sigmoid): Sigmoid()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (16): bottleneck_IR_SE(\n",
              "        (shortcut_layer): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
              "        (res_layer): Sequential(\n",
              "          (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (2): PReLU(num_parameters=256)\n",
              "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (5): SEModule(\n",
              "            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (relu): ReLU(inplace=True)\n",
              "            (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (sigmoid): Sigmoid()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (17): bottleneck_IR_SE(\n",
              "        (shortcut_layer): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
              "        (res_layer): Sequential(\n",
              "          (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (2): PReLU(num_parameters=256)\n",
              "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (5): SEModule(\n",
              "            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (relu): ReLU(inplace=True)\n",
              "            (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (sigmoid): Sigmoid()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (18): bottleneck_IR_SE(\n",
              "        (shortcut_layer): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
              "        (res_layer): Sequential(\n",
              "          (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (2): PReLU(num_parameters=256)\n",
              "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (5): SEModule(\n",
              "            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (relu): ReLU(inplace=True)\n",
              "            (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (sigmoid): Sigmoid()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (19): bottleneck_IR_SE(\n",
              "        (shortcut_layer): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
              "        (res_layer): Sequential(\n",
              "          (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (2): PReLU(num_parameters=256)\n",
              "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (5): SEModule(\n",
              "            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (relu): ReLU(inplace=True)\n",
              "            (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (sigmoid): Sigmoid()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (20): bottleneck_IR_SE(\n",
              "        (shortcut_layer): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
              "        (res_layer): Sequential(\n",
              "          (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (2): PReLU(num_parameters=256)\n",
              "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (5): SEModule(\n",
              "            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (relu): ReLU(inplace=True)\n",
              "            (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (sigmoid): Sigmoid()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (21): bottleneck_IR_SE(\n",
              "        (shortcut_layer): Sequential(\n",
              "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (res_layer): Sequential(\n",
              "          (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (2): PReLU(num_parameters=512)\n",
              "          (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (5): SEModule(\n",
              "            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (relu): ReLU(inplace=True)\n",
              "            (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (sigmoid): Sigmoid()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (22): bottleneck_IR_SE(\n",
              "        (shortcut_layer): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
              "        (res_layer): Sequential(\n",
              "          (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (2): PReLU(num_parameters=512)\n",
              "          (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (5): SEModule(\n",
              "            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (relu): ReLU(inplace=True)\n",
              "            (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (sigmoid): Sigmoid()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (23): bottleneck_IR_SE(\n",
              "        (shortcut_layer): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
              "        (res_layer): Sequential(\n",
              "          (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (2): PReLU(num_parameters=512)\n",
              "          (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (5): SEModule(\n",
              "            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (relu): ReLU(inplace=True)\n",
              "            (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (sigmoid): Sigmoid()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (styles): ModuleList(\n",
              "      (0-2): 3 x GradualStyleBlock(\n",
              "        (convs): Sequential(\n",
              "          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "          (1): LeakyReLU(negative_slope=0.01)\n",
              "          (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "          (3): LeakyReLU(negative_slope=0.01)\n",
              "          (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "          (5): LeakyReLU(negative_slope=0.01)\n",
              "          (6): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "          (7): LeakyReLU(negative_slope=0.01)\n",
              "        )\n",
              "        (linear): EqualLinear(512, 512)\n",
              "      )\n",
              "      (3-6): 4 x GradualStyleBlock(\n",
              "        (convs): Sequential(\n",
              "          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "          (1): LeakyReLU(negative_slope=0.01)\n",
              "          (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "          (3): LeakyReLU(negative_slope=0.01)\n",
              "          (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "          (5): LeakyReLU(negative_slope=0.01)\n",
              "          (6): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "          (7): LeakyReLU(negative_slope=0.01)\n",
              "          (8): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "          (9): LeakyReLU(negative_slope=0.01)\n",
              "        )\n",
              "        (linear): EqualLinear(512, 512)\n",
              "      )\n",
              "      (7-17): 11 x GradualStyleBlock(\n",
              "        (convs): Sequential(\n",
              "          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "          (1): LeakyReLU(negative_slope=0.01)\n",
              "          (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "          (3): LeakyReLU(negative_slope=0.01)\n",
              "          (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "          (5): LeakyReLU(negative_slope=0.01)\n",
              "          (6): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "          (7): LeakyReLU(negative_slope=0.01)\n",
              "          (8): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "          (9): LeakyReLU(negative_slope=0.01)\n",
              "          (10): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "          (11): LeakyReLU(negative_slope=0.01)\n",
              "        )\n",
              "        (linear): EqualLinear(512, 512)\n",
              "      )\n",
              "    )\n",
              "    (latlayer1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (latlayer2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "  )\n",
              "  (decoder): Generator(\n",
              "    (style): Sequential(\n",
              "      (0): PixelNorm()\n",
              "      (1): EqualLinear(512, 512)\n",
              "      (2): EqualLinear(512, 512)\n",
              "      (3): EqualLinear(512, 512)\n",
              "      (4): EqualLinear(512, 512)\n",
              "      (5): EqualLinear(512, 512)\n",
              "      (6): EqualLinear(512, 512)\n",
              "      (7): EqualLinear(512, 512)\n",
              "      (8): EqualLinear(512, 512)\n",
              "    )\n",
              "    (input): ConstantInput()\n",
              "    (conv1): StyledConv(\n",
              "      (conv): ModulatedConv2d(512, 512, 3, upsample=False, downsample=False)\n",
              "      (noise): NoiseInjection()\n",
              "      (activate): FusedLeakyReLU()\n",
              "    )\n",
              "    (to_rgb1): ToRGB(\n",
              "      (conv): ModulatedConv2d(512, 3, 1, upsample=False, downsample=False)\n",
              "    )\n",
              "    (convs): ModuleList(\n",
              "      (0): StyledConv(\n",
              "        (conv): ModulatedConv2d(512, 512, 3, upsample=True, downsample=False)\n",
              "        (noise): NoiseInjection()\n",
              "        (activate): FusedLeakyReLU()\n",
              "      )\n",
              "      (1): StyledConv(\n",
              "        (conv): ModulatedConv2d(512, 512, 3, upsample=False, downsample=False)\n",
              "        (noise): NoiseInjection()\n",
              "        (activate): FusedLeakyReLU()\n",
              "      )\n",
              "      (2): StyledConv(\n",
              "        (conv): ModulatedConv2d(512, 512, 3, upsample=True, downsample=False)\n",
              "        (noise): NoiseInjection()\n",
              "        (activate): FusedLeakyReLU()\n",
              "      )\n",
              "      (3): StyledConv(\n",
              "        (conv): ModulatedConv2d(512, 512, 3, upsample=False, downsample=False)\n",
              "        (noise): NoiseInjection()\n",
              "        (activate): FusedLeakyReLU()\n",
              "      )\n",
              "      (4): StyledConv(\n",
              "        (conv): ModulatedConv2d(512, 512, 3, upsample=True, downsample=False)\n",
              "        (noise): NoiseInjection()\n",
              "        (activate): FusedLeakyReLU()\n",
              "      )\n",
              "      (5): StyledConv(\n",
              "        (conv): ModulatedConv2d(512, 512, 3, upsample=False, downsample=False)\n",
              "        (noise): NoiseInjection()\n",
              "        (activate): FusedLeakyReLU()\n",
              "      )\n",
              "      (6): StyledConv(\n",
              "        (conv): ModulatedConv2d(512, 512, 3, upsample=True, downsample=False)\n",
              "        (noise): NoiseInjection()\n",
              "        (activate): FusedLeakyReLU()\n",
              "      )\n",
              "      (7): StyledConv(\n",
              "        (conv): ModulatedConv2d(512, 512, 3, upsample=False, downsample=False)\n",
              "        (noise): NoiseInjection()\n",
              "        (activate): FusedLeakyReLU()\n",
              "      )\n",
              "      (8): StyledConv(\n",
              "        (conv): ModulatedConv2d(512, 256, 3, upsample=True, downsample=False)\n",
              "        (noise): NoiseInjection()\n",
              "        (activate): FusedLeakyReLU()\n",
              "      )\n",
              "      (9): StyledConv(\n",
              "        (conv): ModulatedConv2d(256, 256, 3, upsample=False, downsample=False)\n",
              "        (noise): NoiseInjection()\n",
              "        (activate): FusedLeakyReLU()\n",
              "      )\n",
              "      (10): StyledConv(\n",
              "        (conv): ModulatedConv2d(256, 128, 3, upsample=True, downsample=False)\n",
              "        (noise): NoiseInjection()\n",
              "        (activate): FusedLeakyReLU()\n",
              "      )\n",
              "      (11): StyledConv(\n",
              "        (conv): ModulatedConv2d(128, 128, 3, upsample=False, downsample=False)\n",
              "        (noise): NoiseInjection()\n",
              "        (activate): FusedLeakyReLU()\n",
              "      )\n",
              "      (12): StyledConv(\n",
              "        (conv): ModulatedConv2d(128, 64, 3, upsample=True, downsample=False)\n",
              "        (noise): NoiseInjection()\n",
              "        (activate): FusedLeakyReLU()\n",
              "      )\n",
              "      (13): StyledConv(\n",
              "        (conv): ModulatedConv2d(64, 64, 3, upsample=False, downsample=False)\n",
              "        (noise): NoiseInjection()\n",
              "        (activate): FusedLeakyReLU()\n",
              "      )\n",
              "      (14): StyledConv(\n",
              "        (conv): ModulatedConv2d(64, 32, 3, upsample=True, downsample=False)\n",
              "        (noise): NoiseInjection()\n",
              "        (activate): FusedLeakyReLU()\n",
              "      )\n",
              "      (15): StyledConv(\n",
              "        (conv): ModulatedConv2d(32, 32, 3, upsample=False, downsample=False)\n",
              "        (noise): NoiseInjection()\n",
              "        (activate): FusedLeakyReLU()\n",
              "      )\n",
              "    )\n",
              "    (upsamples): ModuleList()\n",
              "    (to_rgbs): ModuleList(\n",
              "      (0-3): 4 x ToRGB(\n",
              "        (upsample): Upsample()\n",
              "        (conv): ModulatedConv2d(512, 3, 1, upsample=False, downsample=False)\n",
              "      )\n",
              "      (4): ToRGB(\n",
              "        (upsample): Upsample()\n",
              "        (conv): ModulatedConv2d(256, 3, 1, upsample=False, downsample=False)\n",
              "      )\n",
              "      (5): ToRGB(\n",
              "        (upsample): Upsample()\n",
              "        (conv): ModulatedConv2d(128, 3, 1, upsample=False, downsample=False)\n",
              "      )\n",
              "      (6): ToRGB(\n",
              "        (upsample): Upsample()\n",
              "        (conv): ModulatedConv2d(64, 3, 1, upsample=False, downsample=False)\n",
              "      )\n",
              "      (7): ToRGB(\n",
              "        (upsample): Upsample()\n",
              "        (conv): ModulatedConv2d(32, 3, 1, upsample=False, downsample=False)\n",
              "      )\n",
              "    )\n",
              "    (noises): Module()\n",
              "  )\n",
              "  (face_pool): AdaptiveAvgPool2d(output_size=(256, 256))\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "#@title Setup Repository\n",
        "import os\n",
        "os.chdir('/content')\n",
        "CODE_DIR = 'encoder4editing'\n",
        "\n",
        "!git clone https://github.com/omertov/encoder4editing.git $CODE_DIR\n",
        "!wget https://github.com/ninja-build/ninja/releases/download/v1.8.2/ninja-linux.zip\n",
        "#!sudo unzip ninja-linux.zip -d /usr/local/bin/\n",
        "\n",
        "!sudo update-alternatives --install /usr/bin/ninja ninja /usr/local/bin/ninja 1 --force\n",
        "os.chdir(f'./{CODE_DIR}')\n",
        "\n",
        "from argparse import Namespace\n",
        "import time\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "sys.path.append(\".\")\n",
        "sys.path.append(\"..\")\n",
        "\n",
        "from utils.common import tensor2im\n",
        "from models.psp import pSp  # we use the pSp framework to load the e4e encoder.\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "!pip install gdown\n",
        "\n",
        "file_id = '1cUv_reLE6k3604or78EranS7XzuVMWeO'\n",
        "output_name = 'model.pth'\n",
        "\n",
        "import gdown\n",
        "gdown.download(f'https://drive.google.com/uc?id={file_id}', output_name, quiet=False)\n",
        "\n",
        "import torch\n",
        "\n",
        "model_path = output_name  # Path to the downloaded model file\n",
        "ckpt = torch.load(model_path, map_location='cpu')\n",
        "\n",
        "opts = ckpt['opts']\n",
        "opts['checkpoint_path'] = model_path\n",
        "opts= Namespace(**opts)\n",
        "net = pSp(opts)\n",
        "net.eval()\n",
        "net.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXY_Po50zxMY",
        "outputId": "5c1427d7-3e77-46ff-ce72-f8815e1e6b6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "miM1o-SLw989",
        "outputId": "58453629-5b0f-483b-dcf0-c7b5d96eb3eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (6.2.3)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.5)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy) (0.2.13)\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-itrkhimx\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-itrkhimx\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (6.2.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (24.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.66.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.4.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.19.0+cu121)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2024.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->clip==1.0) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-5TWxJ5zDOLp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0aa9b400-f166-41d7-e373-1d7b2a7146b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-f9c1fc8c0380>:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  g_ema.load_state_dict(torch.load('/content/drive/MyDrive/stylegan2-ffhq-config-f (1).pt')[\"g_ema\"], strict=False)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from model import Generator\n",
        "\n",
        "g_ema = Generator(1024, 512, 8)\n",
        "\n",
        "g_ema.load_state_dict(torch.load('/content/drive/MyDrive/stylegan2-ffhq-config-f (1).pt')[\"g_ema\"], strict=False)\n",
        "g_ema.eval()\n",
        "g_ema = g_ema.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1bcFE2pv4kFP"
      },
      "outputs": [],
      "source": [
        "# Set the random seed\n",
        "#torch.manual_seed(seed)\n",
        "\n",
        "# Assuming g_ema is already defined and initialized\n",
        "mean_latent = g_ema.mean_latent(4023)\n",
        "\n",
        "latent_code_init_not_trunc = torch.randn(1, 512).cuda()\n",
        "with torch.no_grad():\n",
        "    img_orig, latent_code_init = g_ema([latent_code_init_not_trunc], return_latents=True,\n",
        "                                        truncation=0.7, truncation_latent=mean_latent)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "0twRAUGkCFdp"
      },
      "outputs": [],
      "source": [
        "if 'shape_predictor_68_face_landmarks.dat' not in os.listdir():\n",
        "    !wget http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n",
        "    !bzip2 -dk shape_predictor_68_face_landmarks.dat.bz2\n",
        "def run_alignment(image_path):\n",
        "    import dlib\n",
        "    from utils.alignment import align_face\n",
        "    predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
        "    aligned_image = align_face(filepath=image_path, predictor=predictor)\n",
        "    print(\"Aligned image has shape: {}\".format(aligned_image.size))\n",
        "    return aligned_image\n",
        "\n",
        "def encode_path(path):\n",
        "\n",
        "    input_image = run_alignment(path)\n",
        "    transform_img = transforms.Compose([\n",
        "            transforms.Resize((256, 256)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
        "    transformed_image = transform_img(input_image)\n",
        "    image, latent = net(transformed_image.unsqueeze(0).to(\"cuda\").float(), randomize_noise=False, return_latents=True)\n",
        "    return image[0], latent\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cyGdfPTAxw_H",
        "outputId": "a1d4f51f-726b-44b0-aeb1-a08d56dfdc54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-91aba4f83174>:42: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  g_ema.load_state_dict(torch.load('/content/drive/MyDrive/stylegan2-ffhq-config-f (1).pt')[\"g_ema\"], strict=False)\n",
            "ERROR:aiogram.dispatcher.dispatcher:Cause exception while getting updates.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/asyncio/sslproto.py\", line 534, in data_received\n",
            "    ssldata, appdata = self._sslpipe.feed_ssldata(data)\n",
            "  File \"/usr/lib/python3.10/asyncio/sslproto.py\", line 206, in feed_ssldata\n",
            "    self._sslobj.unwrap()\n",
            "  File \"/usr/lib/python3.10/ssl.py\", line 979, in unwrap\n",
            "    return self._sslobj.shutdown()\n",
            "ssl.SSLError: [SSL: APPLICATION_DATA_AFTER_CLOSE_NOTIFY] application data after close notify (_ssl.c:2702)\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/aiogram/bot/api.py\", line 139, in make_request\n",
            "    async with session.post(url, data=req, **kwargs) as response:\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/aiohttp/client.py\", line 1353, in __aenter__\n",
            "    self._resp = await self._coro\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/aiohttp/client.py\", line 684, in _request\n",
            "    await resp.start(conn)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/aiohttp/client_reqrep.py\", line 999, in start\n",
            "    message, payload = await protocol.read()  # type: ignore[union-attr]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/aiohttp/streams.py\", line 640, in read\n",
            "    await self._waiter\n",
            "  File \"/usr/lib/python3.10/asyncio/futures.py\", line 285, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 304, in __wakeup\n",
            "    future.result()\n",
            "  File \"/usr/lib/python3.10/asyncio/futures.py\", line 201, in result\n",
            "    raise self._exception.with_traceback(self._exception_tb)\n",
            "aiohttp.client_exceptions.ClientOSError: [Errno 1] [SSL: APPLICATION_DATA_AFTER_CLOSE_NOTIFY] application data after close notify (_ssl.c:2702)\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/aiogram/dispatcher/dispatcher.py\", line 383, in start_polling\n",
            "    updates = await self.bot.get_updates(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/aiogram/bot/bot.py\", line 97, in get_updates\n",
            "    result = await self.request(api.Methods.GET_UPDATES, payload)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/aiogram/bot/base.py\", line 208, in request\n",
            "    return await api.make_request(self.session, self.server, self.__token, method, data, files,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/aiogram/bot/api.py\", line 142, in make_request\n",
            "    raise exceptions.NetworkError(f\"aiohttp client throws an error: {e.__class__.__name__}: {e}\")\n",
            "aiogram.utils.exceptions.NetworkError: Aiohttp client throws an error: ClientOSError: [Errno 1] [SSL: APPLICATION_DATA_AFTER_CLOSE_NOTIFY] application data after close notify (_ssl.c:2702)\n",
            "WARNING:aiogram:Updates were skipped successfully.\n",
            "WARNING:aiogram.dispatcher.dispatcher:Polling is stopped.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aligned image has shape: (256, 256)\n",
            "torch.Size([1, 18, 512])\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.8330]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0., device='cuda:0', grad_fn=<MulBackward0>) tensor(0., device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.0, loss: 0.8330, epoch:1\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.8330]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0., device='cuda:0', grad_fn=<MulBackward0>) tensor(5.9605e-10, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.1, loss: 0.8330, epoch:2\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7500]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.7336, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0014, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.1, loss: 1.4844, epoch:3\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7485]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.2652, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0012, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.1, loss: 1.0146, epoch:4\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7822]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.0830, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0007, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.1, loss: 0.8657, epoch:5\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.8306]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.2348, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0006, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.1, loss: 1.0664, epoch:6\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.8164]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.3114, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0012, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.1, loss: 1.1289, epoch:7\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7808]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.2444, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0017, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.1, loss: 1.0273, epoch:8\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7373]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.1727, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0022, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.1, loss: 0.9126, epoch:9\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7236]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.1561, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0022, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.1, loss: 0.8823, epoch:10\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7227]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.1849, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0018, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.1, loss: 0.9097, epoch:11\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7480]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.2046, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0011, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.09755282581475767, loss: 0.9536, epoch:12\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7563]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.1881, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0007, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.09045084971874738, loss: 0.9448, epoch:13\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7368]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.1485, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0006, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.07938926261462365, loss: 0.8857, epoch:14\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7295]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.1142, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0006, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.06545084971874737, loss: 0.8442, epoch:15\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7324]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.0926, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0008, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.049999999999999996, loss: 0.8262, epoch:16\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7373]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.0835, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0009, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.03454915028125262, loss: 0.8218, epoch:17\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7344]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.0814, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0009, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.02061073738537635, loss: 0.8169, epoch:18\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7314]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.0807, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0009, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.009549150281252628, loss: 0.8130, epoch:19\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7295]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.0800, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0009, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.0024471741852423235, loss: 0.8105, epoch:20\n",
            "Aligned image has shape: (256, 256)\n",
            "torch.Size([1, 18, 512])\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7754]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0., device='cuda:0', grad_fn=<MulBackward0>) tensor(0., device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.0, loss: 0.7754, epoch:1\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7754]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0., device='cuda:0', grad_fn=<MulBackward0>) tensor(-5.9605e-10, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.1, loss: 0.7754, epoch:2\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7119]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.7344, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0014, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.1, loss: 1.4473, epoch:3\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7056]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.2491, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0013, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.1, loss: 0.9561, epoch:4\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7222]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.0843, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0008, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.1, loss: 0.8076, epoch:5\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7505]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.2585, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0009, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.1, loss: 1.0098, epoch:6\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7397]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.3379, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0014, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.1, loss: 1.0791, epoch:7\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7295]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.2391, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0016, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.1, loss: 0.9702, epoch:8\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7168]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.1205, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0015, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.1, loss: 0.8389, epoch:9\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7139]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.0994, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0012, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.1, loss: 0.8149, epoch:10\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7080]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.1500, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0007, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.1, loss: 0.8584, epoch:11\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7202]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.1835, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0006, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.09755282581475767, loss: 0.9043, epoch:12\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7275]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.1517, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0006, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.09045084971874738, loss: 0.8799, epoch:13\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7412]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.0901, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0005, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.07938926261462365, loss: 0.8315, epoch:14\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7451]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.0470, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0004, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.06545084971874737, loss: 0.7925, epoch:15\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7437]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.0352, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0004, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.049999999999999996, loss: 0.7793, epoch:16\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7451]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.0382, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0004, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.03454915028125262, loss: 0.7837, epoch:17\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7432]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.0414, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0004, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.02061073738537635, loss: 0.7852, epoch:18\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7393]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.0417, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0003, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.009549150281252628, loss: 0.7812, epoch:19\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7378]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.0407, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0003, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.0024471741852423235, loss: 0.7788, epoch:20\n",
            "Aligned image has shape: (256, 256)\n",
            "torch.Size([1, 18, 512])\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7920]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0., device='cuda:0', grad_fn=<MulBackward0>) tensor(0., device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.0, loss: 0.7920, epoch:1\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7915]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0., device='cuda:0', grad_fn=<MulBackward0>) tensor(2.9802e-10, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.1, loss: 0.7915, epoch:2\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7441]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.7331, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0017, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.1, loss: 1.4785, epoch:3\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7290]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.2424, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0014, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.1, loss: 0.9731, epoch:4\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7461]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.0876, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0007, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.1, loss: 0.8350, epoch:5\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7510]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.2743, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0008, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.1, loss: 1.0264, epoch:6\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7451]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.3549, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0016, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.1, loss: 1.1016, epoch:7\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7285]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.2476, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0010, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.1, loss: 0.9771, epoch:8\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7168]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.1301, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0011, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.1, loss: 0.8477, epoch:9\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7124]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.1108, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0012, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.1, loss: 0.8242, epoch:10\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7178]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.1618, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0011, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.1, loss: 0.8804, epoch:11\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7227]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.1995, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0009, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.09755282581475767, loss: 0.9229, epoch:12\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7280]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.1765, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0008, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.09045084971874738, loss: 0.9053, epoch:13\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7334]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.1184, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0007, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.07938926261462365, loss: 0.8521, epoch:14\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7334]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.0756, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0006, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.06545084971874737, loss: 0.8096, epoch:15\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7319]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.0605, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0006, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.049999999999999996, loss: 0.7930, epoch:16\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7324]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.0598, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0005, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.03454915028125262, loss: 0.7925, epoch:17\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7334]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.0611, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0005, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.02061073738537635, loss: 0.7949, epoch:18\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7334]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.0603, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0005, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.009549150281252628, loss: 0.7939, epoch:19\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7324]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.0589, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0004, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.0024471741852423235, loss: 0.7920, epoch:20\n",
            "Aligned image has shape: (256, 256)\n",
            "torch.Size([1, 18, 512])\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7739]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0., device='cuda:0', grad_fn=<MulBackward0>) tensor(0., device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.0, loss: 0.7739, epoch:1\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7739]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0., device='cuda:0', grad_fn=<MulBackward0>) tensor(0., device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.1, loss: 0.7739, epoch:2\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7578]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.7332, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0021, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.1, loss: 1.4932, epoch:3\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7324]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.2811, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0016, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.1, loss: 1.0156, epoch:4\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7422]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.1386, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0011, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.1, loss: 0.8818, epoch:5\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7568]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.3158, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0010, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.1, loss: 1.0732, epoch:6\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7529]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.3634, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0008, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.1, loss: 1.1172, epoch:7\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7544]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.2318, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0008, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.1, loss: 0.9873, epoch:8\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7568]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.0971, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0007, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.1, loss: 0.8545, epoch:9\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7490]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.0750, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0005, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.1, loss: 0.8247, epoch:10\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7461]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.1448, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0006, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.1, loss: 0.8911, epoch:11\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7476]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.1956, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0008, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.09755282581475767, loss: 0.9443, epoch:12\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7461]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.1683, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0008, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.09045084971874738, loss: 0.9155, epoch:13\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7461]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.1024, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0006, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.07938926261462365, loss: 0.8491, epoch:14\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7490]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.0534, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0005, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.06545084971874737, loss: 0.8027, epoch:15\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7520]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.0375, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0004, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.049999999999999996, loss: 0.7900, epoch:16\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7529]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.0391, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0004, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.03454915028125262, loss: 0.7925, epoch:17\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7520]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.0422, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0004, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.02061073738537635, loss: 0.7944, epoch:18\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7510]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.0424, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0004, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.009549150281252628, loss: 0.7939, epoch:19\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7505]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.0414, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0004, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.0024471741852423235, loss: 0.7925, epoch:20\n",
            "Aligned image has shape: (256, 256)\n",
            "torch.Size([1, 18, 512])\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7793]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0., device='cuda:0', grad_fn=<MulBackward0>) tensor(0., device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.0, loss: 0.7793, epoch:1\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7793]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0., device='cuda:0', grad_fn=<MulBackward0>) tensor(0., device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.1, loss: 0.7793, epoch:2\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7646]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.7326, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0021, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.1, loss: 1.4990, epoch:3\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7646]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.2010, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0016, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.1, loss: 0.9673, epoch:4\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7842]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.0621, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0006, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.1, loss: 0.8467, epoch:5\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7983]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.2747, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0009, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.1, loss: 1.0742, epoch:6\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7817]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.3471, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0007, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.1, loss: 1.1299, epoch:7\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7749]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.2246, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0008, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.1, loss: 1.0000, epoch:8\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7690]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.0882, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0008, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.1, loss: 0.8584, epoch:9\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7666]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.0643, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0006, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.1, loss: 0.8315, epoch:10\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7544]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.1363, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0005, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.1, loss: 0.8911, epoch:11\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7544]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.1930, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0006, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.09755282581475767, loss: 0.9478, epoch:12\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7554]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.1708, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0005, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.09045084971874738, loss: 0.9268, epoch:13\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7568]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.1048, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0005, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.07938926261462365, loss: 0.8623, epoch:14\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7607]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.0544, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0004, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.06545084971874737, loss: 0.8154, epoch:15\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7690]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.0364, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0004, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.049999999999999996, loss: 0.8062, epoch:16\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7725]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.0368, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0003, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.03454915028125262, loss: 0.8096, epoch:17\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7720]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.0393, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0003, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.02061073738537635, loss: 0.8120, epoch:18\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7710]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.0394, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0003, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.009549150281252628, loss: 0.8110, epoch:19\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7705]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.0383, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0003, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.0024471741852423235, loss: 0.8091, epoch:20\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.8027]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0., device='cuda:0', grad_fn=<MulBackward0>) tensor(0., device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.0, loss: 0.8027, epoch:1\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.8027]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0., device='cuda:0', grad_fn=<MulBackward0>) tensor(0., device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.1, loss: 0.8027, epoch:2\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7500]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.7335, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0045, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.1, loss: 1.4883, epoch:3\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7402]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.2820, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0038, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.1, loss: 1.0264, epoch:4\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7568]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.0916, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0024, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.1, loss: 0.8511, epoch:5\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7856]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.2444, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0012, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.1, loss: 1.0312, epoch:6\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.8003]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.3312, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0016, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.1, loss: 1.1338, epoch:7\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7808]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.2603, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0020, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.1, loss: 1.0430, epoch:8\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7607]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.1474, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0022, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.1, loss: 0.9102, epoch:9\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7529]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.1033, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0019, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.1, loss: 0.8584, epoch:10\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7534]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.1410, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0017, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.1, loss: 0.8960, epoch:11\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7544]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.1755, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0017, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.09755282581475767, loss: 0.9312, epoch:12\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7617]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.1608, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0016, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.09045084971874738, loss: 0.9238, epoch:13\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7661]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.1167, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0015, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.07938926261462365, loss: 0.8843, epoch:14\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7627]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.0774, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0014, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.06545084971874737, loss: 0.8418, epoch:15\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7573]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.0624, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0014, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.049999999999999996, loss: 0.8213, epoch:16\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7485]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.0613, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0014, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.03454915028125262, loss: 0.8115, epoch:17\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7441]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.0625, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0014, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.02061073738537635, loss: 0.8081, epoch:18\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7412]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.0621, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0015, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.009549150281252628, loss: 0.8047, epoch:19\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7402]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.0609, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0015, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.0024471741852423235, loss: 0.8027, epoch:20\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.8062]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0., device='cuda:0', grad_fn=<MulBackward0>) tensor(0., device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.0, loss: 0.8062, epoch:1\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.8062]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0., device='cuda:0', grad_fn=<MulBackward0>) tensor(0., device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.1, loss: 0.8062, epoch:2\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7397]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.7314, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0028, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.1, loss: 1.4736, epoch:3\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7354]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.2653, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0031, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.1, loss: 1.0039, epoch:4\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7451]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.0992, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0025, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.1, loss: 0.8467, epoch:5\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7744]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.2675, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0016, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.1, loss: 1.0439, epoch:6\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7715]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.3397, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0016, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.1, loss: 1.1133, epoch:7\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7471]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.2477, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0023, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.1, loss: 0.9971, epoch:8\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7432]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.1400, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0025, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.1, loss: 0.8857, epoch:9\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7383]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.1091, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0024, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.1, loss: 0.8501, epoch:10\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7402]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.1530, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0022, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.1, loss: 0.8955, epoch:11\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7412]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.1883, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0019, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.09755282581475767, loss: 0.9316, epoch:12\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7422]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.1636, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0014, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.09045084971874738, loss: 0.9072, epoch:13\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7451]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.1086, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0012, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.07938926261462365, loss: 0.8545, epoch:14\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7480]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.0666, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0013, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.06545084971874737, loss: 0.8159, epoch:15\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7451]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.0524, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0013, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.049999999999999996, loss: 0.7988, epoch:16\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7422]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.0534, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0013, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.03454915028125262, loss: 0.7969, epoch:17\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7407]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.0554, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0013, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.02061073738537635, loss: 0.7974, epoch:18\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7397]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.0545, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0013, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.009549150281252628, loss: 0.7959, epoch:19\n",
            "Loading ResNet ArcFace\n",
            "hi\n",
            "no\n",
            "tensor([[0.7393]], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<RsubBackward1>) tensor(0.0529, device='cuda:0', grad_fn=<MulBackward0>) tensor(0.0012, device='cuda:0', grad_fn=<MulBackward0>)\n",
            "lr: 0.0024471741852423235, loss: 0.7935, epoch:20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:aiogram:Goodbye!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torchvision.utils import make_grid\n",
        "from torchvision.transforms import ToPILImage\n",
        "from aiogram import Bot, Dispatcher, executor, types\n",
        "from io import BytesIO\n",
        "from encoder4editing.models.stylegan2.model import Generator\n",
        "from argparse import Namespace\n",
        "import time\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "import math\n",
        "import torchvision\n",
        "from torch import optim\n",
        "import clip\n",
        "from id_loss import IDLoss\n",
        "from clip_loss import CLIPLoss\n",
        "import nest_asyncio\n",
        "from aiogram.types import ContentType\n",
        "nest_asyncio.apply()\n",
        "args = Namespace()\n",
        "args.description = 'blonde'\n",
        "args.lr_rampup = 0.05\n",
        "args.lr = 0.1\n",
        "args.step = 20\n",
        "args.l1_lambda = 0.005\n",
        "args.l2_lambda = 0.008 # The weight for similarity to the original image.\n",
        "args.save_intermediate_image_every = 1\n",
        "args.results_dir = 'results'\n",
        "args.ir_se50_weights = '/content/drive/MyDrive/model_ir_se50.pth'\n",
        "args.stylegan_size = 1024\n",
        "user_message = \"\"\n",
        "\n",
        "# Initialize the bot and dispatcher\n",
        "bot = Bot(token='7493418929:AAGvEbYAm-8_8F3hVbCkj6pe8_FIFXX5IvA')\n",
        "dp = Dispatcher(bot)\n",
        "\n",
        "\n",
        "\n",
        "# Load the StyleGAN2 model\n",
        "g_ema = Generator(1024, 512, 8)\n",
        "g_ema.load_state_dict(torch.load('/content/drive/MyDrive/stylegan2-ffhq-config-f (1).pt')[\"g_ema\"], strict=False)\n",
        "g_ema.eval()\n",
        "g_ema = g_ema.cuda()\n",
        "\n",
        "clip_loss = CLIPLoss(args)\n",
        "def generate_random_image():\n",
        "    # Set the random seed\n",
        "    #torch.manual_seed(seed)\n",
        "\n",
        "    # Assuming g_ema is already defined and initialized\n",
        "    mean_latent = g_ema.mean_latent(4023)\n",
        "\n",
        "    latent_code_init_not_trunc = torch.randn(1, 512).cuda()\n",
        "    with torch.no_grad():\n",
        "        img_orig, latent_code_init = g_ema([latent_code_init_not_trunc], return_latents=True,\n",
        "                                           truncation=0.7, truncation_latent=mean_latent)\n",
        "\n",
        "    return img_orig, latent_code_init\n",
        "\n",
        "\n",
        "\n",
        "# Command handler for /start and /help\n",
        "@dp.message_handler(commands=['start', 'help'])\n",
        "async def welcome(message: types.Message):\n",
        "    await message.reply(\"Hello, I'm a bot! I will help you to modify an image according some text. Use /random to generate a random image. Also, you can write /upload_image and after that upload a real image.\")\n",
        "\n",
        "img, latent_code_init = 0, 0\n",
        "result_image = 0\n",
        "# Command handler for /logo\n",
        "@dp.message_handler(commands=['random'])\n",
        "async def logo(message: types.Message):\n",
        "    global img, latent_code_init, result_image\n",
        "    img, latent_code_init = generate_random_image()\n",
        "    await message.reply(\"Your randomly generated image:\")\n",
        "    # Convert the tensor to a PIL image\n",
        "    result_image = ToPILImage()(make_grid(img.detach().cpu(), normalize=True, scale_each=True, padding=0))\n",
        "    h, w = result_image.size\n",
        "    resized_image = result_image.resize((h // 2, w // 2))\n",
        "    # Save the image to a BytesIO object\n",
        "    buffer = BytesIO()\n",
        "    resized_image.save(buffer, format='PNG')\n",
        "    buffer.seek(0)\n",
        "    # Send the image\n",
        "\n",
        "    await bot.send_photo(chat_id=message.chat.id, photo=buffer)\n",
        "    await message.reply(\"Please enter your text:\")\n",
        "\n",
        "@dp.message_handler(commands=['upload_image'])\n",
        "async def ask_for_photo(message: types.Message):\n",
        "    await message.reply(\"Please send the image you want to upload.\")\n",
        "\n",
        "\n",
        "@dp.message_handler(content_types=ContentType.PHOTO)\n",
        "async def handle_photo(message: types.Message):\n",
        "    global latent_code_init\n",
        "    # Get the file ID of the highest quality photo\n",
        "    photo = message.photo[-1]\n",
        "    photo_file_id = photo.file_id\n",
        "    # Store the photo file ID in a variable or use it for further processing\n",
        "    file_info = await bot.get_file(photo_file_id)\n",
        "    downloaded_file = await bot.download_file(file_info.file_path)\n",
        "    with open(\"downloaded_image.jpg\", \"wb\") as f:\n",
        "        f.write(downloaded_file.getvalue())\n",
        "\n",
        "    result_image, latent_code_init = encode_path('/content/encoder4editing/downloaded_image.jpg')\n",
        "    print(latent_code_init.shape)\n",
        "    result_image = tensor2im(result_image)\n",
        "    # Save the image to a BytesIO object\n",
        "    buffer = BytesIO()\n",
        "    result_image.save(buffer, format='PNG')\n",
        "    buffer.seek(0)\n",
        "    # Send the image\n",
        "\n",
        "    await message.reply(\"Please enter your text:\")\n",
        "\n",
        "\n",
        "@dp.message_handler()\n",
        "async def handle_message(message: types.Message):\n",
        "    global result_image, latent_code_init\n",
        "    user_message = message.text\n",
        "    await message.reply(f'Please wait until your image will be generated')\n",
        "    # Initialize the latent vector to be updated.\n",
        "\n",
        "\n",
        "    latent = latent_code_init.detach().clone().requires_grad_(True)\n",
        "    latent_code_init = latent_code_init.detach().clone()\n",
        "    optimizer = optim.Adam([latent], lr=args.lr)\n",
        "\n",
        "    def train():\n",
        "        with torch.no_grad():\n",
        "            img_orig, _ = g_ema([latent_code_init], input_is_latent=True, randomize_noise=False)\n",
        "\n",
        "        # The learning rate adjustment function.\n",
        "        def get_lr(t, initial_lr, rampdown=0.50, rampup=0.05):\n",
        "            lr_ramp = min(1, (1 - t) / rampdown)\n",
        "            lr_ramp = 0.5 - 0.5 * math.cos(lr_ramp * math.pi)\n",
        "            lr_ramp = lr_ramp * min(1, t / rampup)\n",
        "\n",
        "            return initial_lr * lr_ramp\n",
        "\n",
        "        text_inputs = torch.cat([clip.tokenize(user_message)]).cuda()\n",
        "        os.makedirs(args.results_dir, exist_ok=True)\n",
        "        t1 = time.time()\n",
        "        k = 0\n",
        "        for i in range(args.step):\n",
        "            # Adjust the learning rate.\n",
        "            t = i / args.step\n",
        "            lr = get_lr(t, args.lr)\n",
        "            optimizer.param_groups[0][\"lr\"] = lr\n",
        "\n",
        "            # Generate an image using the latent vector.\n",
        "            img_gen, _ = g_ema([latent], input_is_latent=True, randomize_noise=False)\n",
        "            # Calculate the loss value.\n",
        "            c_loss = clip_loss(img_gen, text_inputs)\n",
        "            l2_loss = ((latent_code_init - latent) ** 2).sum()\n",
        "\n",
        "            id_loss = IDLoss(args)\n",
        "            print('hi')\n",
        "            i_loss = id_loss(img_gen, img_orig)[0]\n",
        "            print('no')\n",
        "            loss = c_loss + args.l2_lambda * l2_loss + args.l1_lambda * i_loss\n",
        "            print(c_loss, args.l2_lambda * l2_loss, args.l1_lambda * i_loss)\n",
        "            # Get gradient and update the latent vector.\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            k += 1\n",
        "            # Log the current state.\n",
        "            print(f\"lr: {lr}, loss: {loss.item():.4f}, epoch:{k}\")\n",
        "            if args.save_intermediate_image_every > 0 and i % args.save_intermediate_image_every == 0:\n",
        "                with torch.no_grad():\n",
        "                    img_gen, _ = g_ema([latent], input_is_latent=True, randomize_noise=False)\n",
        "                torchvision.utils.save_image(img_gen, f\"results/{str(i).zfill(5)}.png\", normalize=True)\n",
        "            t2 = time.time()\n",
        "\n",
        "\n",
        "        # Display the initial image and result image.\n",
        "        final_result = torch.cat([img_orig, img_gen])\n",
        "        torchvision.utils.save_image(final_result.detach().cpu(), os.path.join(args.results_dir, \"final_result.jpg\"),\n",
        "                                     normalize=True, scale_each=True)\n",
        "        return img_gen\n",
        "\n",
        "    final_result = train()\n",
        "\n",
        "    result_image = ToPILImage()(make_grid(final_result.detach().cpu(), normalize=True, scale_each=True, padding=0))\n",
        "    h, w = result_image.size\n",
        "    result_image.resize((h // 2, w // 2))\n",
        "\n",
        "    await message.reply(\"Result image:\")\n",
        "    buffer = BytesIO()\n",
        "    result_image.save(buffer, format='PNG')  # Save PIL Image to buffer\n",
        "    buffer.seek(0)  # Rewind the buffer to the beginning\n",
        "\n",
        "    # Send the image\n",
        "    await bot.send_photo(chat_id=message.chat.id, photo=buffer)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    executor.start_polling(dp, skip_updates=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vOjOQ3m5HmTK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}